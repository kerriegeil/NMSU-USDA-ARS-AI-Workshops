{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Advanced DL Networks\n",
    "## Laura E. Boucheron, Electrical & Computer Engineering, NMSU\n",
    "### May 2021\n",
    "Copyright (C) 2021  Laura E. Boucheron\n",
    "\n",
    "This information is free; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.\n",
    "\n",
    "This work is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License along with this work; if not, If not, see <https://www.gnu.org/licenses/>.\n",
    "\n",
    "## Overview\n",
    "In this tutorial, we study some more deep learning architectures for both image analysis and time series analysis.\n",
    "\n",
    "This tutorial contains 4 sections:\n",
    "  - **Section 0: Preliminaries**: some notes on using this notebook, how to download the image dataset that we will use for this tutorial, and import commands for the libraries necessary for this tutorial\n",
    "  - **Section 1: Activation Maximization**: A method to generate images that maximally activate a selected neuron\n",
    "  - **Section 2: YOLO-v3 for Object Detection**: A network to detect objects in images \n",
    "  - **Section 3: Style Transfer**: A network to change the \"style\" of an image\n",
    "  - **Section 4: Generative Adversarial Networks (GANs)**: A network that can generate synthetic images similar to a given dataset\n",
    "  \n",
    "There are a few subsections with the heading \"**<span style='color:Green'> Your turn: </span>**\" throughout this tutorial in which you will be asked to apply what you have learned.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0: Preliminaries \n",
    "## A Note on Jupyter Notebooks\n",
    "\n",
    "There are two main types of cells in this notebook: code and markdown (text).  You can add a new cell with the plus sign in the menu bar above and you can change the type of cell with the dropdown menu in the menu bar above.  As you complete this tutorial, you may wish to add additional code cells to try out your own code and markdown cells to add your own comments or notes. \n",
    "\n",
    "Markdown cells can be augmented with a number of text formatting features, including\n",
    "  - bulleted\n",
    "  - lists\n",
    "\n",
    "embedded $\\LaTeX$, monotype specification of `code syntax`, **bold font**, and *italic font*.  There are many other features of markdown cells--see the jupyter documentation for more information.\n",
    "\n",
    "You can edit a cell by double clicking on it.  If you double click on this cell, you can see how to implement the various formatting referenced above.  Code cells can be run and markdown cells can be formatted using Shift+Enter or by selecting the Run button in the toolbar above.\n",
    "\n",
    "Once you have completed (all or part) of this notebook, you can share your results with colleagues by sending them the `.ipynb` file.  Your colleagues can then open the file and will see your markdown and code cells as well as any results that were printed or displayed at the time you saved the notebook.  If you prefer to send a notebook without results displayed (like this notebook appeared when you downloaded it), you can select (\"Restart & Clear Output\") from the Kernel menu above.  You can also export this notebook in a non-executable form, e.g., `.pdf` through the File, Save As menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.1 Downloading Necessary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Section 1, we will need:\n",
    " - The `tf-keras-vis` package.  Follow the installation instructions at https://pypi.org/project/tf-keras-vis/.  \n",
    "\n",
    "In Section 2, we will need:\n",
    " - The YOLO-v3 weights available for download from: https://pjreddie.com/media/files/yolov3.weights (273 MB).  Save this data file to your working directory.\n",
    " - The `zebra.jpg` image available from from https://machinelearningmastery.com/wp-content/uploads/2019/03/zebra.jpg.  Save this file to your working directory.\n",
    " \n",
    "In Section 3, we will need:\n",
    " - The image of Van Gogh's Starry Night painting, available at https://i.imgur.com/9ooB60I.jpg.  Save this file to filename `starry_night.jpg` in your working directory.\n",
    " \n",
    "In Section 4, we will need:\n",
    " - Nothing beyond what we have already encountered in previous tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.1a Import Necessary Libraries (For users using a local machine)\n",
    "Here, at the top of the code, we import all the libraries necessary for this tutorial.  We will introduce the functionality of any new libraries throughout the tutorial, but include all import statements here as standard coding practice.  We include a brief comment after each library here to indicate its main purpose within this tutorial.\n",
    "\n",
    "It would be best to run this next cell before the workshop starts to make sure you have all the necessary packages installed on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np # mathematical and scientific functions\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "from matplotlib.patches import Rectangle # plot rectangles\n",
    "# format matplotlib options\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# Imports for Section 1\n",
    "import tensorflow as tf\n",
    "import tf_keras_vis.activation_maximization\n",
    "import tf_keras_vis.utils.callbacks\n",
    "from matplotlib import cm\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.utils.scores import CategoricalScore\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Imports for Section 2 (some imports from Section 1 may also be required if just running Section 2)\n",
    "# import necessary keras layers; see help files for more information on each layer type\n",
    "from keras.layers import Conv2D # convolutional layer\n",
    "from keras.layers import Input # input layer\n",
    "from keras.layers import BatchNormalization # batchnorm layer to standardize batches and help optimization\n",
    "from keras.layers import LeakyReLU # activation similar to ReLU\n",
    "from keras.layers import ZeroPadding2D # zero pads 2D inputs (images)\n",
    "from keras.layers import UpSampling2D # upsamples 2D inputs by replicating rows and columns of data\n",
    "from keras.layers import Dense # fully connected layer\n",
    "# import layer manipulation functions\n",
    "from keras.layers.merge import add, concatenate # functions to add or concatenate tensors\n",
    "# import models and model related functions; \n",
    "from keras.models import Model # a generic keras model class used to modify architectures\n",
    "from keras.models import Sequential # the basic deep learning model\n",
    "from keras.models import load_model # to load a pre-saved model (may require hdf libraries installed)\n",
    "# import functions to input and preprocess images\n",
    "from keras.preprocessing.image import load_img # keras method to read in images \n",
    "from keras.preprocessing.image import img_to_array # keras method to convert images to numpy array\n",
    "\n",
    "# Imports for Section 3 (some imports from Sections 1 and two may also be required if running just Section 3)\n",
    "from tensorflow import keras\n",
    "import imageio\n",
    "from tensorflow.keras.applications import vgg16\n",
    "\n",
    "# Imports for Section 4 (some imports from Sections 1-3 may also be required if running just Section 4)\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.1b Build the Conda Environment (For users using the ARS HPC Ceres with JupyterLab)\n",
    "Please follow instructions at https://kerriegeil.github.io/NMSU-USDA-ARS-AI-Workshops/setup/#on-the-ceres-hpc\n",
    "\n",
    "You will want to do this BEFORE the workshop starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 Activation Maximization\n",
    "In this part, we will explore visualizations of images that maximally activate a given neuron.  If we use gradient ascent to generate an image that maximally activates a dense layer neuron, we will generate an image that is most class-like according to the network, e.g., the most flamingo-like image.  If we generate an image that maximally activates a convolutional layer neuron, we will generate an image that is most feature-like according to the network, allowing us to explore what features each of the convolutional layer filters are cueing on.  \n",
    " - We will use the `tf-keras-vis` package.  Follow the installation instructions at https://pypi.org/project/tf-keras-vis/.  \n",
    "\n",
    "The examples in this section were adapted from the `tf-keras-vis` documentation at https://pypi.org/project/tf-keras-vis/.  The `tf-keras-vis` package is released under the MIT license:\n",
    "\n",
    "Copyright 2019 k.keisen@gmail.com\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1 Maximal Activation for a Dense Layer Neuron\n",
    "In this part, we visualize the maximal activation for a dense layer neuron.  Since the last dense layer correspond to each of the 1000 ImageNet classes, this will visualize a class as “understood” by the VGG16 network.\n",
    " - We need to load the VGG16 network as a `tensorflow` `Model`.  This requires slightly different syntax that used in our exploration of the VGG16 network in Tutorial 4:\n",
    "   ```\n",
    "   model1=tf.keras.applications.vgg16.VGG16(include_top=True,weights='imagenet')\n",
    "   ```\n",
    " - Define a model modifier that changes the activation of the layer to be visualized to be linear:\n",
    "   ```\n",
    "   def model_modifier(m):\n",
    "       m.layers[-1].activation=tf.keras.activations.linear\n",
    "   ```\n",
    " - Define an activation maximization instance. \n",
    "   ```\n",
    "   model1_am=tf_keras_vis.activation_maximization.ActivationMaximization(model1,model_modifier,clone=True)\n",
    "   ```\n",
    "   Note—if you use `clone=False`, you will use fewer machine resources but you will modify `model1` and will need to reload for the next step.  If you use `clone=True`, you will create a copy of the model instance which will leave `model1` unchanged in memory and use more machine resources.  \n",
    " - Define a loss function associated with an arbitrary category number.  Here, we use category number 130 which corresponds to the class `'flamingo'`.\n",
    "   ```\n",
    "   def loss(output):\n",
    "       return output[:,130]\n",
    "   ```\n",
    " - Now we can compute the maximum activation image corresponding to category 130:\n",
    "   ```\n",
    "   max_act=model1_am(loss,callbacks=[tf_keras_vis.utils.callbacks.Print(interval=50)])\n",
    "   ```\n",
    "   The `callbacks` option prints out various metrics every 50 iterations of the gradient ascent. This option is not necessary but can provide some feedback regarding whether the code is running successfully.  The variable `max_act` will be a classes$\\times$rows$\\times$columns$\\times$channels `ndarray`.  Here, classes=1.\n",
    "\n",
    "The code below will visualize the maximal activation `max_act` for the flamingo class.  Does this image look like a flamingo?  What different aspects of a flamingo can you see in the image?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_modifier(m):\n",
    "    m.layers[-1].activation = tf.keras.activations.linear\n",
    "\n",
    "def loss(output):\n",
    "    return output[:,130]\n",
    "    \n",
    "model1 = tf.keras.applications.vgg16.VGG16(include_top=True,weights='imagenet')\n",
    "model1_am = tf_keras_vis.activation_maximization.ActivationMaximization(model1,\n",
    "                                                 model_modifier,\n",
    "                                                 clone=False)\n",
    "max_act = model1_am(loss,steps=512)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(max_act.squeeze().astype(np.uint8))\n",
    "plt.axis('off')\n",
    "plt.title('flamingo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "Choose another class from ImageNet (see https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a for a handy and not easy to find comprehensive list of ImageNet classes) and visualize the maximal activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_modifier(m):\n",
    "    m.layers[-1].activation = tf.keras.activations.linear\n",
    "\n",
    "def loss(output):\n",
    "    return output[:,550]\n",
    "\n",
    "max_act = model1_am(loss,steps=512)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(max_act.squeeze().astype(np.uint8))\n",
    "plt.axis('off')\n",
    "plt.title('esspresso maker')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2 Maximal Activation for a Convolutional Neuron\n",
    "In this part, we visualize the maximal activation for a convolutional layer neuron.  Since the convolutional layers are understood to be feature extractors, this will visualize an image that embodies the feature extracted by that neuron in the VGG16 network.\n",
    " - If needed (e.g., you used `clone=False`), load the VGG16 network into variable `model1` using the same syntax as in part (g-i).\n",
    " - Define a model modifier that outputs the activation at the desired layer and changes the activation of that layer to be linear:\n",
    "   ```\n",
    "   def model_modifier(current_model):\n",
    "       # using layer name\n",
    "       target_layer=current_model.get_layer(name=layer_name)\n",
    "       # using layer number\n",
    "       #target_layer=current_model.get_layer(index=layer_idx)\n",
    "       new_model=tf.keras.Model(inputs=current_model.inputs,\\\n",
    "                                outputs=target_layer.output)\n",
    "       new_model.layers[-1].activation=tf.keras.activations.linear\n",
    "   ```\n",
    "   Note that there are two options here to specify the target layer by name or by index; be sure to comment out the undesired option.  Note also that this function assuming that you have defined a variable outside of the scope of the function named `layer_name` or `layer_idx`.  For this part, use the last convolutional layer (`layer_name='block1_conv3'`, `layer_idx=17`).\n",
    " - Define an activation maximization instance using the same syntax as in part (g-i).\n",
    " - Define a loss function associated with an arbitrary filter number `filter_num`:\n",
    "   ```\n",
    "   def loss(output):\n",
    "       return output[...,filter_num]\n",
    "   ```\n",
    "   Note that the variable `filter_num` is assumed to be defined outside of the function.  For this part, use filter number 42.\n",
    " - Now we can compute the maximum activation image corresponding to filter number 42 using the same syntax as in part (g-i).\n",
    "      \n",
    "The following code will visualize the maximal activation max_act for filter number 42 of the last convolutional layer.  What feature(s) does this filter seem to cue on?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_modifier(current_model):\n",
    "    # using layer name\n",
    "    target_layer=current_model.get_layer(name=layer_name)\n",
    "    # using layer number\n",
    "    #target_layer=current_model.get_layer(index=layer_idx)\n",
    "    new_model=tf.keras.Model(inputs=current_model.inputs,outputs=target_layer.output)\n",
    "    new_model.layers[-1].activation=tf.keras.activations.linear\n",
    "    return new_model\n",
    "\n",
    "def loss(output):\n",
    "    return output[...,filter_num]\n",
    "\n",
    "model1 = tf.keras.applications.vgg16.VGG16(include_top=True,weights='imagenet')\n",
    "\n",
    "layer_name = 'block5_conv3'\n",
    "model1_am = tf_keras_vis.activation_maximization.ActivationMaximization(model1,\n",
    "                                                 model_modifier,\n",
    "                                                 clone=False)\n",
    "\n",
    "filter_num = 42\n",
    "max_act = model1_am(loss,steps=512)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(max_act.squeeze().astype(np.uint8))\n",
    "plt.axis('off')\n",
    "plt.title('filter'+str(filter_num))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "Choose another filter from the same layer and visualize the maximal activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_modifier(current_model):\n",
    "    # using layer name\n",
    "    target_layer=current_model.get_layer(name=layer_name)\n",
    "    # using layer number\n",
    "    #target_layer=current_model.get_layer(index=layer_idx)\n",
    "    new_model=tf.keras.Model(inputs=current_model.inputs,outputs=target_layer.output)\n",
    "    new_model.layers[-1].activation=tf.keras.activations.linear\n",
    "    return new_model\n",
    "\n",
    "def loss(output):\n",
    "    return output[...,filter_num]\n",
    "\n",
    "model1 = tf.keras.applications.vgg16.VGG16(include_top=True,weights='imagenet')\n",
    "\n",
    "layer_name = 'block5_conv3'\n",
    "model1_am = tf_keras_vis.activation_maximization.ActivationMaximization(model1,\n",
    "                                                 model_modifier,\n",
    "                                                 clone=False)\n",
    "\n",
    "filter_num = 128\n",
    "max_act = model1_am(loss,steps=512)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(max_act.squeeze().astype(np.uint8))\n",
    "plt.axis('off')\n",
    "plt.title('filter'+str(filter_num))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3 GradCAM (Gradient Class Activation Map)\n",
    "We can use the GradCAM method to give further insight into portions of the image that are most important for the classification.  Since we have the CalTech101 dataset downloaded from a previous tutorial, we can use those images as input to this visualization.  We wil have to choose those categories in ImageNet (https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a) that also exist in CalTech101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def model_modifier(cloned_model):\n",
    "    cloned_model.layers[-1].activation = tf.keras.activations.linear\n",
    "    return cloned_model\n",
    "\n",
    "score = CategoricalScore(130) # 130 is flamingo\n",
    "category = 'flamingo'\n",
    "\n",
    "# Load images and Convert them to a Numpy array\n",
    "img1 = np.asarray(load_img('101_ObjectCategories/'+category+'/image_0001.jpg', target_size=(224, 224)))\n",
    "\n",
    "# Preparing input data for VGG16\n",
    "X = preprocess_input(img1)\n",
    "\n",
    "# Show original image\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1)\n",
    "plt.axis('off')\n",
    "plt.title(category)\n",
    "\n",
    "model1 = tf.keras.applications.vgg16.VGG16(include_top=True,weights='imagenet')\n",
    "\n",
    "# Create Gradcam object\n",
    "gradcam = Gradcam(model1,model_modifier=model_modifier,clone=False)\n",
    "\n",
    "# Generate heatmap with GradCAM\n",
    "cam = gradcam(score, X, penultimate_layer=-1)\n",
    "heatmap = np.uint8(cm.jet(cam)[..., :3] * 255)\n",
    "\n",
    "# Show results\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img1)\n",
    "plt.imshow(heatmap.squeeze(),cmap='jet',alpha=0.5) # overlay heatmap\n",
    "plt.axis('off')\n",
    "plt.title(category)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "Choose another class to visualize the GradCAM results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_modifier(cloned_model):\n",
    "    cloned_model.layers[-1].activation = tf.keras.activations.linear\n",
    "    return cloned_model\n",
    "\n",
    "score = CategoricalScore(949) # 949 is strawberry\n",
    "category = 'strawberry'\n",
    "\n",
    "# Load images and Convert them to a Numpy array\n",
    "img1 = np.asarray(load_img('101_ObjectCategories/'+category+'/image_0001.jpg', target_size=(224, 224)))\n",
    "\n",
    "# Preparing input data for VGG16\n",
    "X = preprocess_input(img1)\n",
    "\n",
    "# Show original image\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1)\n",
    "plt.axis('off')\n",
    "plt.title(category)\n",
    "\n",
    "model1 = tf.keras.applications.vgg16.VGG16(include_top=True,weights='imagenet')\n",
    "\n",
    "# Create Gradcam object\n",
    "gradcam = Gradcam(model1,model_modifier=model_modifier,clone=False)\n",
    "\n",
    "# Generate heatmap with GradCAM\n",
    "cam = gradcam(score, X, penultimate_layer=-1)\n",
    "heatmap = np.uint8(cm.jet(cam)[..., :3] * 255)\n",
    "\n",
    "# Show results\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img1)\n",
    "plt.imshow(heatmap.squeeze(),cmap='jet',alpha=0.5) # overlay heatmap\n",
    "plt.axis('off')\n",
    "plt.title(category)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 YOLO-v3 for Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will study the use of the YOLO-v3 (You Only Look Once version 3) network for object detection in images.  The papers describing the YOLO architectures can be found at:\n",
    " - YOLO-v1: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\n",
    " - YOLO-v2: http://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf\n",
    " - YOLO-v3: https://arxiv.org/pdf/1804.02767.pdf\n",
    "\n",
    "The code in this section was taken and modified (slightly) from https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/ and https://github.com/experiencor/keras-yolo3\n",
    "\n",
    "The YOLO-v3 weights can be downloaded from: https://pjreddie.com/media/files/yolov3.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License information for the keras-yolo3 code at https://github.com/experiencor/keras-yolo3\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2017 Ngoc Anh Huynh\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define YOLO-v3 architecture and functions to load YOLO-v3 weights\n",
    "These function definitions contain code to define the YOLO-v3 architecture and also to load weights for the YOLO-v3 architecture from the file linked above (https://pjreddie.com/media/files/yolov3.weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code adapted from https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/\n",
    "# which based the code on https://github.com/experiencor/keras-yolo3 (see MIT License statement above)\n",
    "\n",
    "def _conv_block(inp, convs, skip=True):\n",
    "    x = inp\n",
    "    count = 0\n",
    "    for conv in convs:\n",
    "        if count == (len(convs) - 2) and skip:\n",
    "            skip_connection = x\n",
    "        count += 1\n",
    "        if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n",
    "        x = Conv2D(conv['filter'],\\\n",
    "                   conv['kernel'],\\\n",
    "                   strides=conv['stride'],\\\n",
    "                   padding='valid' if conv['stride'] > 1 else 'same',\\\n",
    "                   name='conv_' + str(conv['layer_idx']),\\\n",
    "                   use_bias=False if conv['bnorm'] else True)(x)\n",
    "        if conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n",
    "        if conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n",
    "    return add([skip_connection, x]) if skip else x\n",
    " \n",
    "def make_yolov3_model():\n",
    "    input_image = Input(shape=(None, None, 3))\n",
    "    # Layer  0 => 4\n",
    "    x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\\\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\\\n",
    "                                  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\\\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n",
    "    # Layer  5 => 8\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\\\n",
    "                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\\\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n",
    "    # Layer  9 => 11\n",
    "    x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\\\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n",
    "    # Layer 12 => 15\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\\\n",
    "                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\\\n",
    "                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n",
    "    # Layer 16 => 36\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\\\n",
    "                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n",
    "    skip_36 = x\n",
    "    # Layer 37 => 40\n",
    "    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\\\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\\\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n",
    "    # Layer 41 => 61\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\\\n",
    "                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n",
    "    skip_61 = x\n",
    "    # Layer 62 => 65\n",
    "    x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\\\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\\\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n",
    "    # Layer 66 => 74\n",
    "    for i in range(3):\n",
    "        x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\\\n",
    "                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n",
    "    # Layer 75 => 79\n",
    "    x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\\\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\\\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\\\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\\\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n",
    "    # Layer 80 => 82\n",
    "    yolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\\\n",
    "                              {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n",
    "    # Layer 83 => 86\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n",
    "    x = UpSampling2D(2)(x)\n",
    "    x = concatenate([x, skip_61])\n",
    "    # Layer 87 => 91\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\\\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\\\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\\\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\\\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n",
    "    # Layer 92 => 94\n",
    "    yolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\\\n",
    "                              {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n",
    "    # Layer 95 => 98\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n",
    "    x = UpSampling2D(2)(x)\n",
    "    x = concatenate([x, skip_36])\n",
    "    # Layer 99 => 106\n",
    "    yolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\\\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\\\n",
    "                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\\\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\\\n",
    "                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\\\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\\\n",
    "                               {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n",
    "    model = Model(input_image, [yolo_82, yolo_94, yolo_106])\n",
    "    return model\n",
    " \n",
    "class WeightReader:\n",
    "    def __init__(self, weight_file):\n",
    "        with open(weight_file, 'rb') as w_f:\n",
    "            major,= struct.unpack('i', w_f.read(4))\n",
    "            minor,= struct.unpack('i', w_f.read(4))\n",
    "            revision, = struct.unpack('i', w_f.read(4))\n",
    "            if (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\n",
    "                w_f.read(8)\n",
    "            else:\n",
    "                w_f.read(4)\n",
    "            transpose = (major > 1000) or (minor > 1000)\n",
    "            binary = w_f.read()\n",
    "        self.offset = 0\n",
    "        self.all_weights = np.frombuffer(binary, dtype='float32')\n",
    " \n",
    "    def read_bytes(self, size):\n",
    "        self.offset = self.offset + size\n",
    "        return self.all_weights[self.offset-size:self.offset]\n",
    " \n",
    "    def load_weights(self, model):\n",
    "        for i in range(106):\n",
    "            try:\n",
    "                conv_layer = model.get_layer('conv_' + str(i))\n",
    "                print(\"loading weights of convolution #\" + str(i))\n",
    "                if i not in [81, 93, 105]:\n",
    "                    norm_layer = model.get_layer('bnorm_' + str(i))\n",
    "                    size = np.prod(norm_layer.get_weights()[0].shape)\n",
    "                    beta  = self.read_bytes(size) # bias\n",
    "                    gamma = self.read_bytes(size) # scale\n",
    "                    mean  = self.read_bytes(size) # mean\n",
    "                    var   = self.read_bytes(size) # variance\n",
    "                    weights = norm_layer.set_weights([gamma, beta, mean, var])\n",
    "                if len(conv_layer.get_weights()) > 1:\n",
    "                    bias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n",
    "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "                    kernel = kernel.transpose([2,3,1,0])\n",
    "                    conv_layer.set_weights([kernel, bias])\n",
    "                else:\n",
    "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "                    kernel = kernel.transpose([2,3,1,0])\n",
    "                    conv_layer.set_weights([kernel])\n",
    "            except ValueError:\n",
    "                print(\"no convolution #\" + str(i))\n",
    " \n",
    "    def reset(self):\n",
    "        self.offset = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instantiation of the YOLO-v3 architecture with weights\n",
    "The following code instantiates a YOLO-v3 model and loads the weights pre-trained on the MSCOCO dataset (https://cocodataset.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the following code adapted from https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/\n",
    "# which based the code on https://github.com/experiencor/keras-yolo3 (see MIT License statement above)\n",
    "\n",
    "# create a YOLOv3 Keras model and save it to file\n",
    "# define the model\n",
    "model_yolo3 = make_yolov3_model()\n",
    "# load the model weights\n",
    "weight_reader = WeightReader('yolov3.weights')\n",
    "# set the model weights into the model\n",
    "weight_reader.load_weights(model_yolo3)\n",
    "# compile the model\n",
    "model_yolo3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# save the model to file for ease of use later\n",
    "model_yolo3.save('yolov3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What sort of structure does YOLO-v3 have?\n",
    "We use the `summary` method to display some information about the structure of the YOLO-v3 architecture.  We notice that there are a lot more layers in YOLO-v3 than we saw even in VGG16 and that there are some new kinds of layers that we haven't encountered yet:\n",
    " - **Batch Normalization** - This is a means to smooth out the statistical variations that are encountered from batch to batch in the learning process\n",
    " - **LeakyReLU** - Instead of pegging all negative values to 0 which can cause gradient issues when optimizing, negative values are pegged to some small value.  Thus, negative values are \"leaking\" through the activation.\n",
    " - **ZeroPadding2D** - Pads around the edge(s) of the image with zeros.\n",
    " - **Add** - Adds the output tensors from two layers.\n",
    " - **Concatenate** - Concatenates two tensors.\n",
    " - **UpSampling2D** - Increases the spatial dimensionality of the activations.\n",
    " \n",
    "Many of these additional layers are necessary for the \"skip connections\" in the YOLO architecture.  For one illustration of the skip connection and the concatenation afterwards, see Figure 4 in \n",
    "L. Varela, L. E. Boucheron, N. Malone, and N. Spurlock, “Streak detection in wide field of view images using Convolutional Neural Networks (CNNs),” In proceedings: The Advanced Maui Optical and Space Surveillance Technologies Conference (AMOS), 2019. available: https://amostech.com/TechnicalPapers/2019/Machine-Learning-for-SSA-Applications/Varela.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_yolo3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to preprocess an image for input to YOLO-v3\n",
    "The following function defines a method to preprocess an image into the size expected by the YOLO-v3 network.  It also normalizes the intensities of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code adapted from https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/\n",
    "# which based the code on https://github.com/experiencor/keras-yolo3 (see MIT License statement above)\n",
    "\n",
    "# load and prepare an image\n",
    "def load_image_pixels(filename, shape):\n",
    "    # load the image to get its shape\n",
    "    image = load_img(filename)\n",
    "    width, height = image.size\n",
    "    # load the image with the required size\n",
    "    image = load_img(filename, target_size=shape)\n",
    "    # convert to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # scale pixel values to [0, 1]\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0\n",
    "    # add a dimension so that we have one sample\n",
    "    image = np.expand_dims(image, 0)\n",
    "    return image, width, height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an image and detect objects\n",
    "The code below loads in an example image `zebra.jpg` available from https://machinelearningmastery.com/wp-content/uploads/2019/03/zebra.jpg, preprocess it using the `load_image_pixels` function and sends that image through the YOLO-v3 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code adapted from https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/\n",
    "# which based the code on https://github.com/experiencor/keras-yolo3 (see MIT License statement above)\n",
    "\n",
    "# load yolov3 model and perform object detection\n",
    "\n",
    "# load yolov3 model\n",
    "model_yolo3 = load_model('yolov3.h5')\n",
    "# define the expected input shape for the model\n",
    "input_w, input_h = 416, 416\n",
    "# define our new photo\n",
    "photo_filename = 'zebra.jpg'\n",
    "# load and prepare image\n",
    "image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))\n",
    "# make prediction\n",
    "yhat = model_yolo3.predict(image)\n",
    "# summarize the shape of the list of arrays\n",
    "print('The output from YOLO-v3 is a list of arrays of the following shapes')\n",
    "print([a.shape for a in yhat])\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image.squeeze()) # we actually do need the squeeze here...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we plot the processed image, we note that the `load_image_pixels` function appears to simply reshape the image to the desired dimensions without condideration for the aspect ratio of the image.\n",
    "\n",
    "We also notice that the prediction from YOLO-v3 is a list of arrays.  We need to somehow interpret those arrays in order to understand what has been predicted.  The following functions provide the means to interpret those arrays in terms of what objects have been detected and where in the image they have been detected.\n",
    "\n",
    "### Functions to decode YOLO-v3 output and plot object detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code adapted from https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/\n",
    "# which based the code on https://github.com/experiencor/keras-yolo3 (see MIT License statement above)\n",
    " \n",
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        self.objness = objness\n",
    "        self.classes = classes\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    " \n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    " \n",
    "        return self.label\n",
    " \n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    " \n",
    "        return self.score\n",
    " \n",
    "def _sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    " \n",
    "def decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n",
    "    grid_h, grid_w = netout.shape[:2]\n",
    "    nb_box = 3\n",
    "    netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
    "    nb_class = netout.shape[-1] - 5\n",
    "    boxes = []\n",
    "    netout[..., :2]  = _sigmoid(netout[..., :2])\n",
    "    netout[..., 4:]  = _sigmoid(netout[..., 4:])\n",
    "    netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n",
    "    netout[..., 5:] *= netout[..., 5:] > obj_thresh\n",
    " \n",
    "    for i in range(grid_h*grid_w):\n",
    "        row = i / grid_w\n",
    "        col = i % grid_w\n",
    "        for b in range(nb_box):\n",
    "            # 4th element is objectness score\n",
    "            objectness = netout[int(row)][int(col)][b][4]\n",
    "            if(objectness.all() <= obj_thresh): continue\n",
    "            # first 4 elements are x, y, w, and h\n",
    "            x, y, w, h = netout[int(row)][int(col)][b][:4]\n",
    "            x = (col + x) / grid_w # center position, unit: image width\n",
    "            y = (row + y) / grid_h # center position, unit: image height\n",
    "            w = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n",
    "            h = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n",
    "            # last elements are class probabilities\n",
    "            classes = netout[int(row)][col][b][5:]\n",
    "            box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
    "            boxes.append(box)\n",
    "    return boxes\n",
    " \n",
    "def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n",
    "    new_w, new_h = net_w, net_h\n",
    "    for i in range(len(boxes)):\n",
    "        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
    "        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
    "        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
    "        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
    "        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
    "        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n",
    "        \n",
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x3\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
    "    intersect = intersect_w * intersect_h\n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    return float(intersect) / union\n",
    " \n",
    "def do_nms(boxes, nms_thresh):\n",
    "    if len(boxes) > 0:\n",
    "        nb_class = len(boxes[0].classes)\n",
    "    else:\n",
    "        return\n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "            if boxes[index_i].classes[c] == 0: continue\n",
    "            for j in range(i+1, len(sorted_indices)):\n",
    "                index_j = sorted_indices[j]\n",
    "                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
    "                    boxes[index_j].classes[c] = 0\n",
    "\n",
    "# load and prepare an image\n",
    "def load_image_pixels(filename, shape):\n",
    "    # load the image to get its shape\n",
    "    image = load_img(filename)\n",
    "    width, height = image.size\n",
    "    # load the image with the required size\n",
    "    image = load_img(filename, target_size=shape)\n",
    "    # convert to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # scale pixel values to [0, 1]\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0\n",
    "    # add a dimension so that we have one sample\n",
    "    image = np.expand_dims(image, 0)\n",
    "    return image, width, height\n",
    " \n",
    "# get all of the results above a threshold\n",
    "def get_boxes(boxes, labels, thresh):\n",
    "    v_boxes, v_labels, v_scores = list(), list(), list()\n",
    "    # enumerate all boxes\n",
    "    for box in boxes:\n",
    "        # enumerate all possible labels\n",
    "        for i in range(len(labels)):\n",
    "            # check if the threshold for this label is high enough\n",
    "            if box.classes[i] > thresh:\n",
    "                v_boxes.append(box)\n",
    "                v_labels.append(labels[i])\n",
    "                v_scores.append(box.classes[i]*100)\n",
    "                # don't break, many labels may trigger for one box\n",
    "    return v_boxes, v_labels, v_scores\n",
    " \n",
    "# draw all results\n",
    "def draw_boxes(filename, v_boxes, v_labels, v_scores):\n",
    "    # load the image\n",
    "    data = plt.imread(filename)\n",
    "    # plot the image\n",
    "    plt.imshow(data)\n",
    "    # get the context for drawing boxes\n",
    "    ax = plt.gca()\n",
    "    # plot each box\n",
    "    for i in range(len(v_boxes)):\n",
    "        box = v_boxes[i]\n",
    "        # get coordinates\n",
    "        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
    "        # calculate width and height of the box\n",
    "        width, height = x2 - x1, y2 - y1\n",
    "        # create the shape\n",
    "        rect = Rectangle((x1, y1), width, height, fill=False, color='white')\n",
    "        # draw the box\n",
    "        ax.add_patch(rect)\n",
    "        # draw text and score in top left corner\n",
    "        label = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n",
    "        plt.text(x1, y1, label, color='white')\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Display detection results for example image\n",
    "The anchors defined in the code below are associated with the anchor boxes used in the YOLO networks.  These anchor boxes essentially define expected aspect ratios for objects in images and are used to \"anchor\" the detected bounding boxes.  The anchor boxes defined here are selected as good for the MSCOCO dataset.\n",
    "\n",
    "There is also a `class_threshold` specified below.  This threshold changes the tolerance for accepting an object detection.  If we make this smaller, objects with less confidence will be included in the prediction.\n",
    "\n",
    "There is additionally a threshold (the second parameters in the `do_nms` function call) for the non-maxima suppression of the boxes.  If we increase that value, we will find more overlapping (and potentially conflicting) bounding boxes in the prediction.\n",
    "\n",
    "Finally, there are the list of 80 objects from the MSCOCO dataset which will be used to annotate the object detections on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code adapted from https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/\n",
    "# which based the code on https://github.com/experiencor/keras-yolo3 (see MIT License statement above)\n",
    "\n",
    "# load yolov3 model and perform object detection\n",
    "\n",
    "# load yolov3 model\n",
    "model_yolo3 = load_model('yolov3.h5')\n",
    "# define the expected input shape for the model\n",
    "input_w, input_h = 416, 416\n",
    "# define our new photo\n",
    "photo_filename = 'zebra.jpg'\n",
    "# load and prepare image\n",
    "image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))\n",
    "# make prediction\n",
    "yhat = model_yolo3.predict(image)\n",
    "\n",
    "# define the anchors\n",
    "anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n",
    "# define the probability threshold for detected objects\n",
    "class_threshold = 0.6\n",
    "boxes = list()\n",
    "for i in range(len(yhat)):\n",
    "    # decode the output of the network\n",
    "    boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)\n",
    "# correct the sizes of the bounding boxes for the shape of the image\n",
    "correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)\n",
    "# suppress non-maximal boxes\n",
    "do_nms(boxes, 0.5)\n",
    "# define the labels\n",
    "labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n",
    "    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n",
    "    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n",
    "    \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n",
    "    \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
    "    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
    "    \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
    "    \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\n",
    "    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n",
    "    \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n",
    "# get the details of the detected objects\n",
    "v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n",
    "# summarize what we found\n",
    "for i in range(len(v_boxes)):\n",
    "    print(v_labels[i], v_scores[i])\n",
    "# draw what we found\n",
    "draw_boxes(photo_filename, v_boxes, v_labels, v_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the network correctly predicted the presence of and location of the three zebras in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "Modify the code above to see how the network behaves on different images, or with different parameter choices, especially for the `class_threshold` or the `do_nms` threshold.  For your convenience, the code from the cell above has been copied down below for you to modify.  The `photo_filename` has been defined here to use the `peppers.png` image from Tutorial 1.  The method will not correctly identify items in the `peppers.png` image since it has not been taught to recognize any of the objects within the peppers image.  It may not correctly identify objects in other images that you show it.  Remember that the only objects that this network knows, because of the objects that it has been trained on the MSCOCO dataset, are the 80 objects defined in the `labels` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code adapted from https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/\n",
    "# which based the code on https://github.com/experiencor/keras-yolo3 (see MIT License statement above)\n",
    "\n",
    "# load yolov3 model and perform object detection\n",
    "\n",
    "# load yolov3 model\n",
    "model_yolo3 = load_model('yolov3.h5')\n",
    "# define the expected input shape for the model\n",
    "input_w, input_h = 416, 416\n",
    "# define our new photo\n",
    "photo_filename = 'peppers.png'\n",
    "# load and prepare image\n",
    "image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))\n",
    "# make prediction\n",
    "yhat = model_yolo3.predict(image)\n",
    "\n",
    "# define the anchors\n",
    "anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n",
    "# define the probability threshold for detected objects\n",
    "class_threshold = 0.1\n",
    "boxes = list()\n",
    "for i in range(len(yhat)):\n",
    "    # decode the output of the network\n",
    "    boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)\n",
    "# correct the sizes of the bounding boxes for the shape of the image\n",
    "correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)\n",
    "# suppress non-maximal boxes\n",
    "do_nms(boxes, 0.9)\n",
    "# define the labels\n",
    "labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n",
    "    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n",
    "    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n",
    "    \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n",
    "    \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
    "    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
    "    \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
    "    \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\n",
    "    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n",
    "    \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n",
    "# get the details of the detected objects\n",
    "v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n",
    "# summarize what we found\n",
    "for i in range(len(v_boxes)):\n",
    "    print(v_labels[i], v_scores[i])\n",
    "# draw what we found\n",
    "draw_boxes(photo_filename, v_boxes, v_labels, v_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Style Transfer\n",
    "This example adapted from https://keras.io/examples/generative/neural_style_transfer/.  \n",
    "\n",
    "In style transfer, the \"style\" of a given image (the style reference image as defined in the code below) is learned and can be applied to another image (the base image as defined in the code below).  The content of the base image is retained, but is rendered according to the style of the style reference image.  This network will deal with three losses: a style loss (penalizing outputs that are too far in appearance from the characteristics of the style reference image), a content loss (penalizing outputs that are too far in appearance from the content of the base image), and a spatial continuity loss (penalizing outputs that do not maintain the local coherence of the base image).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Images\n",
    "Here, we specify the style image (Van Gogh's Starry Night painting) and the base image as the peppers image from Tutorial 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_image_path = 'peppers.png'\n",
    "base_image = np.asarray(imageio.imread('peppers.png'))\n",
    "style_reference_image_path = 'starry_night.jpg'\n",
    "style_image = np.asarray(imageio.imread('starry_night.jpg'))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(base_image)\n",
    "plt.axis('off')\n",
    "plt.title('Base image')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(style_image)\n",
    "plt.axis('off')\n",
    "plt.title('Style image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions\n",
    "The following function definitions will allow us to preprocess the input image, deprocess the output image, and define and compute the three loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    # Util function to open, resize and format pictures into appropriate tensors\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        image_path, target_size=(img_nrows, img_ncols)\n",
    "    )\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg16.preprocess_input(img)\n",
    "    return tf.convert_to_tensor(img)\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Util function to convert a tensor into a valid image\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n",
    "\n",
    "# The gram matrix of an image tensor (feature-wise outer product)\n",
    "\n",
    "def gram_matrix(x):\n",
    "    x = tf.transpose(x, (2, 0, 1))\n",
    "    features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    "    gram = tf.matmul(features, tf.transpose(features))\n",
    "    return gram\n",
    "\n",
    "# The \"style loss\" is designed to maintain\n",
    "# the style of the reference image in the generated image.\n",
    "# It is based on the gram matrices (which capture style) of\n",
    "# feature maps from the style reference image\n",
    "# and from the generated image\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
    "\n",
    "# An auxiliary loss function\n",
    "# designed to maintain the \"content\" of the\n",
    "# base image in the generated image\n",
    "\n",
    "def content_loss(base, combination):\n",
    "    return tf.reduce_sum(tf.square(combination - base))\n",
    "\n",
    "# The 3rd loss function, total variation loss,\n",
    "# designed to keep the generated image locally coherent\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    a = tf.square(\n",
    "        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, 1:, : img_ncols - 1, :]\n",
    "    )\n",
    "    b = tf.square(\n",
    "        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, : img_nrows - 1, 1:, :]\n",
    "    )\n",
    "    return tf.reduce_sum(tf.pow(a + b, 1.25))\n",
    "\n",
    "def compute_loss(combination_image, base_image, style_reference_image):\n",
    "    input_tensor = tf.concat(\n",
    "        [base_image, style_reference_image, combination_image], axis=0\n",
    "    )\n",
    "    features = feature_extractor(input_tensor)\n",
    "\n",
    "    # Initialize the loss\n",
    "    loss = tf.zeros(shape=())\n",
    "\n",
    "    # Add content loss\n",
    "    layer_features = features[content_layer_name]\n",
    "    base_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    loss = loss + content_weight * content_loss(\n",
    "        base_image_features, combination_features\n",
    "    )\n",
    "    # Add style loss\n",
    "    for layer_name in style_layer_names:\n",
    "        layer_features = features[layer_name]\n",
    "        style_reference_features = layer_features[1, :, :, :]\n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "        sl = style_loss(style_reference_features, combination_features)\n",
    "        loss += (style_weight / len(style_layer_names)) * sl\n",
    "\n",
    "    # Add total variation loss\n",
    "    loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def compute_loss_and_grads(combination_image, base_image, style_reference_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(combination_image, base_image, style_reference_image)\n",
    "    grads = tape.gradient(loss, combination_image)\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use VGG Model as Base\n",
    "We will use the VGG16 model pre-trained on ImageNet as the base network here.  There are some choices here about which portions of the network to use to compute the style and content losses, the size of the generated output image, the weights for each of the losses, and how many iterations to run the optimization for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a VGG16 model loaded with pre-trained ImageNet weights\n",
    "model1 = vgg16.VGG16(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model1.layers])\n",
    "\n",
    "# Set up a model that returns the activation values for every layer in\n",
    "# VGG16 (as a dict).\n",
    "feature_extractor = keras.Model(inputs=model1.inputs, outputs=outputs_dict)\n",
    "\n",
    "# List of layers to use for the style loss.\n",
    "style_layer_names = [\n",
    "    \"block1_conv1\",\n",
    "    \"block2_conv1\",\n",
    "    \"block3_conv1\",\n",
    "    \"block4_conv1\",\n",
    "    \"block5_conv1\",\n",
    "]\n",
    "# The layer to use for the content loss.\n",
    "content_layer_name = \"block5_conv2\"\n",
    "\n",
    "optimizer = keras.optimizers.SGD(\n",
    "    keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n",
    "    )\n",
    ")\n",
    "\n",
    "# Dimensions of the generated picture.\n",
    "width, height = keras.preprocessing.image.load_img(base_image_path).size\n",
    "img_nrows = 400\n",
    "img_ncols = int(width * img_nrows / height)\n",
    "\n",
    "base_image = preprocess_image(base_image_path)\n",
    "style_reference_image = preprocess_image(style_reference_image_path)\n",
    "combination_image = tf.Variable(preprocess_image(base_image_path))\n",
    "\n",
    "# Weights of the different loss components\n",
    "total_variation_weight = 1e-6 # maintains spatial coherence of the base image\n",
    "style_weight = 1e-6 # maintains the style of the style reference image\n",
    "content_weight = 2.5e-8 # maintains the content of the base image\n",
    "\n",
    "iterations = 4000\n",
    "for i in range(1, iterations + 1):\n",
    "    loss, grads = compute_loss_and_grads(\n",
    "        combination_image, base_image, style_reference_image\n",
    "    )\n",
    "    optimizer.apply_gradients([(grads, combination_image)])\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
    "        img = deprocess_image(combination_image.numpy())\n",
    "        fname = \"Iteration_%d.png\" % i\n",
    "        keras.preprocessing.image.save_img(fname, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at Output\n",
    "Note that the code above saves the style transferred image after every 100 iterations.  Let's look at the output image as the network converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 4000\n",
    "img_out = imageio.imread('Iteration_'+str(iteration)+'.png')\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img_out)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So... These Pictures are Cool and All, but Why Would I Care About Style Transfer in Research?\n",
    "Style transfer is just one example of networks taht can operate on an image and output another image.  Another variety of such networks are called Fully Convolutional Neural Networks (FCNNs).  \n",
    "\n",
    "FCNNs can be used for **image segmentation**, i.e., outputting a mask of pixels corresponding to different objects within an image. See https://arxiv.org/abs/1411.4038 for the seminal paper on FCNNs for image segmentation and https://arxiv.org/abs/1505.04597 for a related architecture called the UNet.\n",
    "\n",
    "FCNNs have also recently used for **modality transfer**, e.g., to infer what a fluorescence microscopy image would look like from a transmitted light microscopy image.  See https://www.biorxiv.org/content/10.1101/289504v3.full for an example of modality transfer using a UNet type architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Generative Adversarial Networks (GANs)\n",
    "Generative adversarial networks (GANs) can be used to generate synthetic data that (at least according to the machine) are indistinguishable from real images.  GANs have two components: a discriminator network that classifies (discriminates) between synthetic and real images and a generator network that generates synthetic images.  These two networks compete with each other.  The generator wants to fool the discriminator and the discriminator wants to avoid being fooled.  The better one network becomes, the better the other must become.\n",
    "\n",
    "For this example, we will train a GAN that generates synthetic images of digits similar to the MNIST data.  This example was slightly modified from the example at https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Discriminator Model\n",
    "Let's first define the discriminator.  Note that the discriminator model looks very similar to the MNIST network we worked with in Tutorials 3 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(28,28,1)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Generator Model\n",
    "Let's now define the generator model.  This model is unlike what we have seen before, and uses different layers than we have seen before, but is still a form of a CNN.  In this case, the network is generating a 28x28 image.  We also define a network that combines the discriminator with the generator that we will use to update the generator.  This combined model uses a frozen discriminator as input to the generator.  The generator will update on the bases of the discriminator's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    # upsample to 14x14\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 28x28\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
    "    return model\n",
    " \n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Functions\n",
    "Now we define some further functions helpful for manipulating real and synthetic data and to train the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare mnist training images\n",
    "def load_real_samples():\n",
    "    # load mnist dataset\n",
    "    (trainX, _), (_, _) = load_data()\n",
    "    # expand to 3d, e.g. add channels dimension\n",
    "    X = np.expand_dims(trainX, axis=-1)\n",
    "    # convert from unsigned ints to floats\n",
    "    X = X.astype('float32')\n",
    "    # scale from [0,255] to [0,1]\n",
    "    X = X / 255.0\n",
    "    return X\n",
    " \n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X = dataset[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "# create and save a plot of generated images (reversed grayscale)\n",
    "def save_plot(examples, epoch, n=10):\n",
    "    # plot images\n",
    "    for i in range(n * n):\n",
    "        # define subplot\n",
    "        plt.subplot(n, n, 1 + i)\n",
    "        # turn off axis\n",
    "        plt.axis('off')\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(examples[i, :, :, 0], cmap='gray')\n",
    "    # save plot to file\n",
    "    filename = 'generated_plot_e%03d.png' % (epoch+1)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# evaluate the discriminator, plot generated images, save generator model\n",
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
    "    # prepare real samples\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    # save plot\n",
    "    save_plot(x_fake, epoch)\n",
    "    # save the generator model tile file\n",
    "    filename = 'generator_model_%03d.h5' % (epoch + 1)\n",
    "    g_model.save(filename)\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=256):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            # generate 'fake' examples\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            # create training set for the discriminator\n",
    "            X, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
    "            # update discriminator model weights\n",
    "            d_loss, _ = d_model.train_on_batch(X, y)\n",
    "            # prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = np.ones((n_batch, 1))\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
    "        # evaluate the model performance, sometimes\n",
    "        if ((i+1) % 10 == 0) or i==0:\n",
    "            summarize_performance(i, g_model, d_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network\n",
    "Now we train the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator\n",
    "d_model = define_discriminator()\n",
    "# create the generator\n",
    "g_model = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "# load image data\n",
    "dataset = load_real_samples()\n",
    "# train model\n",
    "train(g_model, d_model, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Output\n",
    "Note in the `summarize_performance` function above that a plot of results is saved every 10 epochs.  We can visualize the quality of they synthetic data as the network trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = '010'\n",
    "img = np.asarray(imageio.imread('generated_plot_e'+epoch+'.png'))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img,cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Epoch '+epoch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
